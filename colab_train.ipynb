{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3OoBmI1vcBEB"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "bsAZ_PhF15Ts",
    "outputId": "3d3fff76-0d24-4a8c-ef46-66b5cc071d72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'rl'...\n",
      "remote: Enumerating objects: 18, done.\u001b[K\n",
      "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
      "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
      "remote: Total 18 (delta 6), reused 16 (delta 4), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (18/18), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/manorzvi/rl.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "S88O_Raj2JUo",
    "outputId": "05765e1f-4da7-4a25-dbf4-67ba634d413b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content/drive/My Drive/DRL/DDDQN\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'/content/drive/My Drive/DRL/DDDQN'\n",
    "models_path   = os.path.join(path,'models')\n",
    "logs_path     = os.path.join(path,'logs')\n",
    "if load:\n",
    "    load_path = os.path.join(models_path, 'BreakoutDeterministic-v4', 'episode-3400__07-31-20_16:04:56.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "LNE9o2Ivs-pf",
    "outputId": "24245ec3-3b14-433e-c265-30a060fba8a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/DRL/DDDQN/models\n",
      "/content/drive/My Drive/DRL/DDDQN/logs\n",
      "/content/drive/My Drive/DRL/DDDQN/models/BreakoutDeterministic-v4/episode-3400__07-31-20_16:04:56.pt\n"
     ]
    }
   ],
   "source": [
    "print(models_path)\n",
    "assert os.path.exists(models_path)\n",
    "print(logs_path)\n",
    "assert os.path.exists(logs_path)\n",
    "if load:\n",
    "    print(load_path)\n",
    "    assert os.path.exists(load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "function ClickConnect(){\n",
    "console.log(\"Working\");\n",
    "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
    "}setInterval(ClickConnect,60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "BcuEeF3N2yIn",
    "outputId": "3d28125a-eff1-440c-a8c7-b05614d39406"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=64, env_id='BreakoutDeterministic-v4', episodes_number=5000, epsilon_decay=1e-05, epsilon_end=0.01, epsilon_start=1.0, experience_replay_capacity=100000, experience_replay_pretrain_size=100000, gamma=0.99, learning_rate=0.00025, load=True, logs='/content/drive/My Drive/DRL/DDDQN/logs', models='/content/drive/My Drive/DRL/DDDQN/models', path='/content/drive/My Drive/DRL/DDDQN/models/BreakoutDeterministic-v4/episode-3400__07-31-20_16:04:56.pt', play=False, save_model_interval=100, target_update_interval=10, train=True)\n",
      "device: cuda\n",
      "env: <TimeLimit<AtariEnv<BreakoutDeterministic-v4>>>, action_space: Discrete(4), observation_space: Box(210, 160, 3)\n",
      "Action Space Size:  4\n",
      "/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py:125: UserWarning: \n",
      "Tesla T4 with CUDA capability sm_75 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the Tesla T4 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n",
      "[I] - Models Directory: /content/drive/My Drive/DRL/DDDQN/models/BreakoutDeterministic-v4\n",
      "[I] - Logs Directory: /content/drive/My Drive/DRL/DDDQN/logs/BreakoutDeterministic-v4\n",
      "2020-08-01 05:20:27.741485: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "[I] Load Model ... Done.\n",
      "|----------|\n",
      "|Online Net|\n",
      "|----------|\n",
      "DDDQNet(\n",
      "  (conv1): Conv2d(4, 16, kernel_size=(5, 5), stride=(2, 2))\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2))\n",
      "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2))\n",
      "  (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (value_fc): Linear(in_features=1120, out_features=512, bias=True)\n",
      "  (value): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (advantage_fc): Linear(in_features=1120, out_features=512, bias=True)\n",
      "  (advantage): Linear(in_features=512, out_features=4, bias=True)\n",
      ")\n",
      "|----------|\n",
      "|Target Net|\n",
      "|----------|\n",
      "DDDQNet(\n",
      "  (conv1): Conv2d(4, 16, kernel_size=(5, 5), stride=(2, 2))\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2))\n",
      "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2))\n",
      "  (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (value_fc): Linear(in_features=1120, out_features=512, bias=True)\n",
      "  (value): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (advantage_fc): Linear(in_features=1120, out_features=512, bias=True)\n",
      "  (advantage): Linear(in_features=512, out_features=4, bias=True)\n",
      ")\n",
      "|----------|\n",
      "|   Loss   |\n",
      "|----------|\n",
      "<function smooth_l1_loss at 0x7f81fa3ad9d8>\n",
      "|---------|\n",
      "|Optimizer|\n",
      "|---------|\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.00025\n",
      "    weight_decay: 0\n",
      ")\n",
      "Pretrain Filling Experience Replay Memory ... 186 214 193 162 130 157 167 319 240 266 129 178 142 159 169 136 239 283 139 139 127 127 227 144 141 135 140 160 134 146 139 322 134 136 279 164 286 165 132 211 132 262 128 136 182 124 278 157 209 155 182 127 167 177 131 247 256 216 135 135 255 210 251 240 213 143 206 132 207 206 136 135 202 140 156 282 164 206 158 129 142 165 134 179 131 145 163 158 148 131 257 183 137 144 203 129 165 167 133 130 190 145 213 162 129 211 160 203 147 224 181 157 139 160 177 164 247 125 179 157 158 213 129 219 137 145 172 129 138 146 185 251 201 128 130 146 158 136 204 137 168 139 180 168 129 180 159 201 181 179 134 132 249 144 198 181 247 182 181 136 155 136 283 128 142 130 207 156 239 137 157 130 134 141 136 203 216 210 253 217 164 184 255 133 218 205 184 174 318 182 135 243 137 140 148 132 175 220 155 169 133 130 225 310 141 139 170 180 159 181 141 133 230 130 284 131 207 141 170 190 153 156 129 141 244 135 177 200 130 128 167 142 159 134 209 160 171 210 161 351 136 221 145 344 140 192 160 129 144 215 204 180 196 154 169 203 173 212 188 142 200 175 130 248 205 247 156 132 145 205 165 145 212 171 219 333 201 135 338 239 160 205 139 166 146 130 129 315 182 203 164 216 207 140 130 183 128 196 154 200 181 132 185 137 311 132 315 211 134 162 180 191 254 182 136 165 180 141 171 133 209 133 187 134 144 200 132 190 169 203 233 283 133 226 140 186 225 176 243 171 201 237 191 126 147 222 131 175 124 127 197 147 139 127 180 132 169 152 134 133 160 197 200 265 143 163 247 259 140 257 162 156 136 170 181 163 209 182 131 128 154 180 125 139 241 138 140 167 314 206 211 215 227 131 237 256 320 130 130 207 177 193 145 162 175 147 225 137 163 147 186 314 266 172 240 155 212 206 208 211 164 188 222 181 200 150 150 126 159 125 138 136 128 129 304 209 132 151 214 161 226 128 133 162 188 184 241 206 142 215 157 272 168 151 206 255 128 175 167 187 164 165 133 186 287 226 175 233 147 210 143 322 131 136 141 146 161 129 131 223 135 181 159 208 285 203 146 133 127 201 168 172 131 166 161 203 173 145 135 146 132 209 179 159 295 133 186 239 191 162 181 139 179 134 130 138 185 155 248 144 261 177 123 133 142 175 166 252 163 224 139 158 125 132 143 177 207 137 179 221 394 216 136 146 169 172 210 130 167 141 142 132 169 141 153 201 126 145 145 138 138 210 161 188 Done.\n",
      "Episode 0 Done.Length: 202.Total Reward: 2.0.Avg Loss: 0.15273773009553918.Action Entropy: 1.3728813381672305.\n",
      "[I] - Update Traget Net ... Done.\n",
      "[I] - Save Model ... Done.\n",
      "Episode 1 Done.Length: 125.Total Reward: 0.0.Avg Loss: 0.10841447221381324.Action Entropy: 1.3763479306965642.\n",
      "Episode 2 Done.Length: 154.Total Reward: 0.0.Avg Loss: 0.10664912868411311.Action Entropy: 1.374142097726892.\n",
      "Episode 3 Done.Length: 128.Total Reward: 0.0.Avg Loss: 0.10449272512580997.Action Entropy: 1.373935075607006.\n",
      "Episode 4 Done.Length: 176.Total Reward: 1.0.Avg Loss: 0.0942651924208901.Action Entropy: 1.3797507172524857.\n",
      "Episode 5 Done.Length: 254.Total Reward: 3.0.Avg Loss: 0.09757945374676995.Action Entropy: 1.38473780749045.\n",
      "Episode 6 Done.Length: 158.Total Reward: 1.0.Avg Loss: 0.09400968258298419.Action Entropy: 1.3505859514099463.\n",
      "Episode 7 Done.Length: 184.Total Reward: 2.0.Avg Loss: 0.09914827915663654.Action Entropy: 1.3744961865751015.\n",
      "Episode 8 Done.Length: 137.Total Reward: 0.0.Avg Loss: 0.09953897820272739.Action Entropy: 1.3745069335488285.\n",
      "Episode 9 Done.Length: 130.Total Reward: 0.0.Avg Loss: 0.09249996044139826.Action Entropy: 1.375388522945514.\n",
      "Episode 10 Done.Length: 306.Total Reward: 4.0.Avg Loss: 0.09368650937992898.Action Entropy: 1.38253379829709.\n",
      "[I] - Update Traget Net ... Done.\n",
      "Episode 11 Done.Length: 135.Total Reward: 0.0.Avg Loss: 0.08702485155834652.Action Entropy: 1.37061766034773.\n",
      "Episode 12 Done.Length: 139.Total Reward: 0.0.Avg Loss: 0.08161087525742394.Action Entropy: 1.384467744674945.\n",
      "Episode 13 Done.Length: 166.Total Reward: 1.0.Avg Loss: 0.08063679249834514.Action Entropy: 1.385669855771611.\n",
      "Episode 14 Done.Length: 175.Total Reward: 1.0.Avg Loss: 0.07860109802674163.Action Entropy: 1.3814696204625974.\n",
      "Episode 15 Done.Length: 177.Total Reward: 1.0.Avg Loss: 0.08716903233461165.Action Entropy: 1.371497955872575.\n",
      "Episode 16 Done.Length: 164.Total Reward: 1.0.Avg Loss: 0.09202794559638608.Action Entropy: 1.383371203479077.\n",
      "Episode 17 Done.Length: 132.Total Reward: 0.0.Avg Loss: 0.08013897025047388.Action Entropy: 1.3765994682011304.\n",
      "Episode 18 Done.Length: 138.Total Reward: 0.0.Avg Loss: 0.0811583546104191.Action Entropy: 1.3822119466252736.\n",
      "Episode 19 Done.Length: 228.Total Reward: 2.0.Avg Loss: 0.08155954021586341.Action Entropy: 1.377023736980784.\n",
      "Episode 20 Done.Length: 140.Total Reward: 0.0.Avg Loss: 0.08610773513238903.Action Entropy: 1.3829700217636633.\n",
      "[I] - Update Traget Net ... Done.\n",
      "Episode 21 Done.Length: 142.Total Reward: 0.0.Avg Loss: 0.07553225667118192.Action Entropy: 1.3854432607277283.\n",
      "Episode 22 Done.Length: 133.Total Reward: 0.0.Avg Loss: 0.07350750649764912.Action Entropy: 1.3848450798852185.\n",
      "Episode 23 Done.Length: 225.Total Reward: 2.0.Avg Loss: 0.07161950127971647.Action Entropy: 1.383307887949916.\n",
      "Episode 24 Done.Length: 130.Total Reward: 0.0.Avg Loss: 0.07697072750284471.Action Entropy: 1.3838250439104602.\n",
      "Episode 25 Done.Length: 413.Total Reward: 7.0.Avg Loss: 0.07430078107008828.Action Entropy: 1.3818830345066968.\n",
      "Episode 26 Done.Length: 207.Total Reward: 2.0.Avg Loss: 0.070002389462808.Action Entropy: 1.3818910640339652.\n",
      "Episode 27 Done.Length: 270.Total Reward: 3.0.Avg Loss: 0.06552871605690665.Action Entropy: 1.3858915374735121.\n",
      "Episode 28 Done.Length: 187.Total Reward: 1.0.Avg Loss: 0.07651093265814508.Action Entropy: 1.3839381337877763.\n",
      "Episode 29 Done.Length: 132.Total Reward: 0.0.Avg Loss: 0.07498396432405352.Action Entropy: 1.3713810177718113.\n",
      "Episode 30 Done.Length: 244.Total Reward: 3.0.Avg Loss: 0.07399003234387813.Action Entropy: 1.3818020603623857.\n",
      "[I] - Update Traget Net ... Done.\n",
      "Episode 31 Done.Length: 138.Total Reward: 0.0.Avg Loss: 0.06894136931773999.Action Entropy: 1.35723053848136.\n",
      "Episode 32 Done.Length: 135.Total Reward: 0.0.Avg Loss: 0.07862643175758421.Action Entropy: 1.3819605754898865.\n",
      "Episode 33 Done.Length: 193.Total Reward: 1.0.Avg Loss: 0.07182209747699425.Action Entropy: 1.384777945348316.\n",
      "Episode 34 Done.Length: 254.Total Reward: 3.0.Avg Loss: 0.06764807667510182.Action Entropy: 1.384062107304179.\n",
      "Episode 35 Done.Length: 183.Total Reward: 1.0.Avg Loss: 0.06763956508012084.Action Entropy: 1.3705219696855127.\n",
      "Episode 36 Done.Length: 133.Total Reward: 0.0.Avg Loss: 0.05617069617024998.Action Entropy: 1.380675752746048.\n",
      "Episode 37 Done.Length: 161.Total Reward: 1.0.Avg Loss: 0.063575575289167.Action Entropy: 1.3724065912685228.\n",
      "Episode 38 Done.Length: 135.Total Reward: 0.0.Avg Loss: 0.057687902809394634.Action Entropy: 1.378520787495388.\n",
      "Episode 39 Done.Length: 137.Total Reward: 0.0.Avg Loss: 0.0599246920639838.Action Entropy: 1.374789493646882.\n",
      "Episode 40 Done.Length: 294.Total Reward: 4.0.Avg Loss: 0.0633059284255161.Action Entropy: 1.384626167096339.\n",
      "[I] - Update Traget Net ... Done.\n",
      "Episode 41 Done.Length: 263.Total Reward: 3.0.Avg Loss: 0.07478423115698564.Action Entropy: 1.3679312041957026.\n",
      "Episode 42 Done.Length: 183.Total Reward: 1.0.Avg Loss: 0.06961091701447478.Action Entropy: 1.379266189179509.\n",
      "Episode 43 Done.Length: 126.Total Reward: 0.0.Avg Loss: 0.07062446723479456.Action Entropy: 1.3747327183129898.\n",
      "Episode 44 Done.Length: 180.Total Reward: 1.0.Avg Loss: 0.0640744374066756.Action Entropy: 1.3860058420276764.\n",
      "Episode 45 Done.Length: 235.Total Reward: 3.0.Avg Loss: 0.06507854078407005.Action Entropy: 1.3832132960498948.\n",
      "Episode 46 Done.Length: 382.Total Reward: 6.0.Avg Loss: 0.06644524020621889.Action Entropy: 1.3812411431894793.\n",
      "Episode 47 Done.Length: 136.Total Reward: 0.0.Avg Loss: 0.06708425556030805.Action Entropy: 1.3802285686886393.\n",
      "Episode 48 Done.Length: 174.Total Reward: 1.0.Avg Loss: 0.06362793710614954.Action Entropy: 1.3845372170125552.\n",
      "Episode 49 Done.Length: 127.Total Reward: 0.0.Avg Loss: 0.05834366108319955.Action Entropy: 1.3840938654749317.\n",
      "Episode 50 Done.Length: 175.Total Reward: 1.0.Avg Loss: 0.06269692489877343.Action Entropy: 1.3745973723924625.\n",
      "[I] - Update Traget Net ... Done.\n",
      "Episode 51 Done.Length: 197.Total Reward: 2.0.Avg Loss: 0.07333660953574711.Action Entropy: 1.3622758355730764.\n",
      "Episode 52 Done.Length: 280.Total Reward: 3.0.Avg Loss: 0.07353367473850576.Action Entropy: 1.3842645336861328.\n",
      "Episode 53 Done.Length: 135.Total Reward: 0.0.Avg Loss: 0.07190063234199495.Action Entropy: 1.380606042503802.\n",
      "Episode 54 Done.Length: 125.Total Reward: 0.0.Avg Loss: 0.06129915703324571.Action Entropy: 1.3836125657666483.\n",
      "Episode 55 Done.Length: 181.Total Reward: 1.0.Avg Loss: 0.06599105724164255.Action Entropy: 1.3779270832345996.\n",
      "Episode 56 Done.Length: 205.Total Reward: 2.0.Avg Loss: 0.06623729726173055.Action Entropy: 1.3752128435680815.\n",
      "Episode 57 Done.Length: 146.Total Reward: 0.0.Avg Loss: 0.07271248719902063.Action Entropy: 1.355816444690593.\n",
      "Episode 58 Done.Length: 246.Total Reward: 3.0.Avg Loss: 0.07183689828057159.Action Entropy: 1.3815396576321524.\n",
      "Episode 59 Done.Length: 129.Total Reward: 0.0.Avg Loss: 0.06534792228530233.Action Entropy: 1.3820866443335331.\n",
      "Episode 60 Done.Length: 137.Total Reward: 0.0.Avg Loss: 0.06642074666350432.Action Entropy: 1.3733100519688253.\n",
      "[I] - Update Traget Net ... Done.\n",
      "Episode 61 Done.Length: 126.Total Reward: 0.0.Avg Loss: 0.05457255854381351.Action Entropy: 1.3779037495441315.\n",
      "Episode 62 Done.Length: 207.Total Reward: 2.0.Avg Loss: 0.060656343828528546.Action Entropy: 1.378520061462451.\n",
      "Episode 63 Done.Length: 250.Total Reward: 3.0.Avg Loss: 0.0576641213749925.Action Entropy: 1.3837058353423812.\n",
      "Episode 64 Done.Length: 281.Total Reward: 3.0.Avg Loss: 0.06223459811947236.Action Entropy: 1.3820100951656402.\n",
      "Episode 65 Done.Length: 275.Total Reward: 3.0.Avg Loss: 0.06204012298511098.Action Entropy: 1.3769595248556037.\n",
      "Episode 66 Done.Length: 197.Total Reward: 2.0.Avg Loss: 0.05561914560243939.Action Entropy: 1.3848381472306814.\n",
      "Episode 67 Done.Length: 295.Total Reward: 4.0.Avg Loss: 0.06245671312412801.Action Entropy: 1.3812127978452586.\n",
      "Episode 68 Done.Length: 207.Total Reward: 2.0.Avg Loss: 0.06187666197701429.Action Entropy: 1.3804057309415383.\n",
      "Episode 69 Done.Length: 128.Total Reward: 0.0.Avg Loss: 0.05957682186723218.Action Entropy: 1.3744371043473338.\n",
      "Episode 70 Done.Length: 154.Total Reward: 1.0.Avg Loss: 0.05939700954984273.Action Entropy: 1.3828018158850555.\n",
      "[I] - Update Traget Net ... Done.\n",
      "Episode 71 Done.Length: 134.Total Reward: 0.0.Avg Loss: 0.06531405085352836.Action Entropy: 1.3839707545616622.\n",
      "Episode 72 Done.Length: 278.Total Reward: 4.0.Avg Loss: 0.06074567408531263.Action Entropy: 1.3732249766436617.\n",
      "Episode 73 Done.Length: 133.Total Reward: 0.0.Avg Loss: 0.052724761096065616.Action Entropy: 1.3765974160869754.\n",
      "Episode 74 Done.Length: 177.Total Reward: 1.0.Avg Loss: 0.06401286886617792.Action Entropy: 1.3686165611995174.\n",
      "Episode 75 Done.Length: 131.Total Reward: 0.0.Avg Loss: 0.06857897996704912.Action Entropy: 1.3858351540024205.\n",
      "Episode 76 Done.Length: 139.Total Reward: 0.0.Avg Loss: 0.05610505807479577.Action Entropy: 1.3774893057806235.\n",
      "Episode 77 Done.Length: 187.Total Reward: 1.0.Avg Loss: 0.05626352664647981.Action Entropy: 1.3821971507976607.\n",
      "Episode 78 Done.Length: 163.Total Reward: 1.0.Avg Loss: 0.06055586398919908.Action Entropy: 1.3833159157426125.\n",
      "Episode 79 Done.Length: 126.Total Reward: 0.0.Avg Loss: 0.058782291363954076.Action Entropy: 1.3801992403692136.\n",
      "Episode 80 Done.Length: 175.Total Reward: 1.0.Avg Loss: 0.06352245361829939.Action Entropy: 1.3816310852323737.\n",
      "[I] - Update Traget Net ... Done.\n",
      "Episode 81 Done.Length: 145.Total Reward: 0.0.Avg Loss: 0.06365756695605304.Action Entropy: 1.379844110852564.\n",
      "Episode 82 Done.Length: 206.Total Reward: 2.0.Avg Loss: 0.058120150483496814.Action Entropy: 1.3724688991968919.\n",
      "Episode 83 Done.Length: 201.Total Reward: 2.0.Avg Loss: 0.05444867582991719.Action Entropy: 1.384294507510256.\n",
      "Episode 84 Done.Length: 130.Total Reward: 0.0.Avg Loss: 0.06298575810519338.Action Entropy: 1.3857365580654515.\n",
      "Episode 85 Done.Length: 222.Total Reward: 2.0.Avg Loss: 0.062166507126170424.Action Entropy: 1.378635164384084.\n",
      "Episode 86 Done.Length: 146.Total Reward: 0.0.Avg Loss: 0.0565255731344223.Action Entropy: 1.3821579861033881.\n",
      "Episode 87 Done.Length: 207.Total Reward: 2.0.Avg Loss: 0.05694798630429432.Action Entropy: 1.3749298047746532.\n",
      "Episode 88 Done.Length: 148.Total Reward: 0.0.Avg Loss: 0.05713955452353162.Action Entropy: 1.3764942098844537.\n",
      "Episode 89 Done.Length: 178.Total Reward: 1.0.Avg Loss: 0.05499981205348529.Action Entropy: 1.3836211368467013.\n",
      "Episode 90 Done.Length: 142.Total Reward: 0.0.Avg Loss: 0.05943366476836113.Action Entropy: 1.3759096886159774.\n",
      "[I] - Update Traget Net ... Done.\n",
      "Episode 91 Done.Length: 141.Total Reward: 0.0.Avg Loss: 0.05936425404859261.Action Entropy: 1.3818893718724583.\n",
      "Episode 92 Done.Length: 164.Total Reward: 1.0.Avg Loss: 0.05339735467269113.Action Entropy: 1.3844924775025302.\n",
      "Episode 93 Done.Length: 130.Total Reward: 0.0.Avg Loss: 0.05764890245325465.Action Entropy: 1.3852806521389063.\n",
      "Episode 94 Done.Length: 231.Total Reward: 2.0.Avg Loss: 0.05313807239370613.Action Entropy: 1.3766119743321377.\n",
      "Episode 95 Done.Length: 247.Total Reward: 3.0.Avg Loss: 0.05617860978799722.Action Entropy: 1.3789590787426218.\n",
      "Episode 96 Done.Length: 144.Total Reward: 0.0.Avg Loss: 0.05366278843761518.Action Entropy: 1.3745053856110416.\n",
      "Episode 97 Done.Length: 230.Total Reward: 2.0.Avg Loss: 0.05343593430838415.Action Entropy: 1.3813305688946373.\n",
      "Episode 98 Done.Length: 267.Total Reward: 4.0.Avg Loss: 0.05739172128265473.Action Entropy: 1.381552892642054.\n",
      "Episode 99 Done.Length: 144.Total Reward: 0.0.Avg Loss: 0.04721071453572347.Action Entropy: 1.3824941260375765.\n",
      "Episode 100 Done.Length: 149.Total Reward: 0.0.Avg Loss: 0.05270799333540102.Action Entropy: 1.3804970284436515.\n",
      "[I] - Update Traget Net ... Done.\n",
      "[I] - Save Model ... Done.\n",
      "Episode 101 Done.Length: 144.Total Reward: 0.0.Avg Loss: 0.05202737427477179.Action Entropy: 1.376398292309105.\n",
      "Episode 102 Done.Length: 184.Total Reward: 2.0.Avg Loss: 0.05637059203676275.Action Entropy: 1.3857805065115498.\n",
      "Episode 103 Done.Length: 127.Total Reward: 0.0.Avg Loss: 0.05384783048793906.Action Entropy: 1.379628851665469.\n",
      "Episode 104 Done.Length: 127.Total Reward: 0.0.Avg Loss: 0.057119565120956395.Action Entropy: 1.361932667741854.\n",
      "Episode 105 Done.Length: 172.Total Reward: 1.0.Avg Loss: 0.058815444658128165.Action Entropy: 1.3757251591666233.\n",
      "Episode 106 Done.Length: 247.Total Reward: 3.0.Avg Loss: 0.05619632010348141.Action Entropy: 1.3756450589852571.\n",
      "Episode 107 Done.Length: 188.Total Reward: 1.0.Avg Loss: 0.055959531823025335.Action Entropy: 1.373785056638448.\n",
      "Episode 108 Done.Length: 208.Total Reward: 2.0.Avg Loss: 0.05072534813336208.Action Entropy: 1.383350155040588.\n",
      "Episode 109 Done.Length: 164.Total Reward: 1.0.Avg Loss: 0.04702903897021756.Action Entropy: 1.3808790302191696.\n",
      "Episode 110 Done.Length: 147.Total Reward: 0.0.Avg Loss: 0.050025460058571516.Action Entropy: 1.3817214611642676.\n",
      "[I] - Update Traget Net ... Done.\n",
      "Episode 111 Done.Length: 129.Total Reward: 0.0.Avg Loss: 0.05010691665542814.Action Entropy: 1.3664346740249353.\n",
      "Episode 112 Done.Length: 158.Total Reward: 1.0.Avg Loss: 0.05375622198829111.Action Entropy: 1.3542643488239963.\n",
      "Episode 113 Done.Length: 273.Total Reward: 3.0.Avg Loss: 0.055422942113321626.Action Entropy: 1.3846393745300558.\n",
      "Episode 114 Done.Length: 135.Total Reward: 0.0.Avg Loss: 0.05554134896545507.Action Entropy: 1.3813640277166286.\n",
      "Episode 115 Done.Length: 147.Total Reward: 0.0.Avg Loss: 0.046687039210008004.Action Entropy: 1.3838868377479645.\n",
      "Episode 116 Done.Length: 129.Total Reward: 0.0.Avg Loss: 0.05113640000972037.Action Entropy: 1.379218049457375.\n",
      "Episode 117 Done.Length: 127.Total Reward: 0.0.Avg Loss: 0.049790553814091254.Action Entropy: 1.3862943611198906.\n",
      "Episode 118 Done.Length: 199.Total Reward: 2.0.Avg Loss: 0.0512353982240893.Action Entropy: 1.363101727553182.\n",
      "Episode 119 Done.Length: 212.Total Reward: 2.0.Avg Loss: 0.05137319843166731.Action Entropy: 1.3830751760914861.\n",
      "Episode 120 Done.Length: 140.Total Reward: 0.0.Avg Loss: 0.05336809497828602.Action Entropy: 1.3848168888223815.\n",
      "[I] - Update Traget Net ... Done.\n",
      "Episode 121 Done.Length: 130.Total Reward: 0.0.Avg Loss: 0.04743961136048759.Action Entropy: 1.3761047551545278.\n",
      "Episode 122 Done.Length: 168.Total Reward: 1.0.Avg Loss: 0.05799796612459351.Action Entropy: 1.3843880873555385.\n",
      "Episode 123 Done.Length: 275.Total Reward: 3.0.Avg Loss: 0.048625715299630945.Action Entropy: 1.377993861929861.\n",
      "Episode 124 Done.Length: 135.Total Reward: 0.0.Avg Loss: 0.0528117546619957.Action Entropy: 1.3834392665685524.\n",
      "Episode 125 Done.Length: 128.Total Reward: 0.0.Avg Loss: 0.053657687742032056.Action Entropy: 1.379037148101454.\n",
      "Episode 126 Done.Length: 181.Total Reward: 1.0.Avg Loss: 0.050025074978123654.Action Entropy: 1.3529495879516906.\n",
      "Episode 127 Done.Length: 160.Total Reward: 1.0.Avg Loss: 0.04946780389302081.Action Entropy: 1.3854752250000342.\n",
      "Episode 128 Done.Length: 206.Total Reward: 2.0.Avg Loss: 0.04803764796703334.Action Entropy: 1.3782069711372502.\n",
      "Episode 129 Done.Length: 175.Total Reward: 1.0.Avg Loss: 0.04747584238595499.Action Entropy: 1.3752077529683107.\n",
      "Episode 130 Done.Length: 137.Total Reward: 0.0.Avg Loss: 0.052236686683381384.Action Entropy: 1.3529069355132701.\n",
      "[I] - Update Traget Net ... Done.\n",
      "Episode 131 Done.Length: 232.Total Reward: 2.0.Avg Loss: 0.053849668713694225.Action Entropy: 1.3776233897362444.\n",
      "Episode 132 Done.Length: 266.Total Reward: 3.0.Avg Loss: 0.04733082659242584.Action Entropy: 1.3836424041198037.\n",
      "Episode 133 Done.Length: 177.Total Reward: 1.0.Avg Loss: 0.052551642981138125.Action Entropy: 1.3830610829908694.\n",
      "Episode 134 Done.Length: 169.Total Reward: 1.0.Avg Loss: 0.04624299802269567.Action Entropy: 1.378518559340519.\n",
      "Episode 135 Done.Length: 210.Total Reward: 2.0.Avg Loss: 0.04685952042348648.Action Entropy: 1.3831895820599927.\n",
      "Episode 136 Done.Length: 127.Total Reward: 0.0.Avg Loss: 0.04967524088715436.Action Entropy: 1.3731132895095073.\n",
      "Episode 137 Done.Length: 213.Total Reward: 2.0.Avg Loss: 0.04951979052797656.Action Entropy: 1.3832639931957718.\n",
      "Episode 138 Done.Length: 172.Total Reward: 1.0.Avg Loss: 0.05130932536083839.Action Entropy: 1.3706662825569884.\n",
      "Episode 139 Done.Length: 232.Total Reward: 3.0.Avg Loss: 0.047914106620659515.Action Entropy: 1.3806696802391065.\n"
     ]
    }
   ],
   "source": [
    "if load:\n",
    "    !python \"rl/DDDQN.py\" -train -exp_rep_cap=100000 -exp_rep_pre=100000 -save_mdl_int=100 -bs=64 -epi_num=5000 -models='$models_path' -logs='$logs_path' -env_id='BreakoutDeterministic-v4' -load -path='$load_path'\n",
    "else:\n",
    "    !python \"rl/DDDQN.py\" -train -exp_rep_cap=100000 -exp_rep_pre=100000 -save_mdl_int=100 -bs=64 -epi_num=5000 -models='$models_path' -logs='$logs_path' -env_id='BreakoutDeterministic-v4'        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aqhti9UUDas-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "DDDQN_colab_train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
